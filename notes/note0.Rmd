---
title: "Evaluation of Bayesian models: Bayes Factors, and out-of-sample generalization^[This note was written in RMarkdown and the source document is available at https://www.somewhere.com and therein one can obtain all the R code to generate the simulations presented here.]"
author: "Mark Andrews"
date: "June 2018"
output: pdf_document
header-includes:
   - \input{include}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(magrittr)
library(pander)
set.seed(10101)
```

In a typical data analysis problem, we have observed data, e.g. $\data = x_1, x_2 \ldots x_i \ldots x_n$, which we assume is generated from a true data generating process $f_{\textrm{True}}$. For example, in the case of independent observations, we would have $x_i \sim f_{\textrm{True}}(x_i)$ for all $i \in 1\ldots n$, with $f_{\textrm{True}}(x_i)$ being the probability density function[^density_function_footnote] for the random variable $x_i$. In general, we do not know and can not know what $f_{\textrm{True}}$ is.

[^density_function_footnote]: Given that a probability mass function can be represented by a density function, we will use probability density function generically to also refer to mass functions.

In order to find the best model of $f_{\textrm{True}}$, we can stipulate a set of candidate models. For simplicity, we'll consider just two candidate models $\mathcal{M}_0$, $\mathcal{M}_1$. These can be defined in general as 
$$
M_0\colon \quad \left.
\begin{aligned}
x_i &\sim f_0(x_i\given \theta_0),\quad\text{for $i \in 1\ldots n$},\\
\theta_0 &\sim \pi_0(\theta\given \omega_0),
\end{aligned}
\right.
$$
and
$$
M_1\colon \quad \left.
\begin{aligned}
x_i &\sim f_1(x_i\given \theta_1),\quad\text{for $i \in 1\ldots n$},\\
\theta_1 &\sim \pi_1(\theta_1\given \omega_1).
\end{aligned}
\right.
$$
In the case of $\mathcal{M}_0$, $f_0$ is a density function over each $x_i$ that is parametrized by $\theta_0$, and $\pi_0$ is a density function over $\theta_0$ by that is (hyper-)parametrized by $\omega_0$. For $\mathcal{M}_1$, $f_1$ is a density function over each $x_i$ that is parametrized by $\theta_1$, and $\pi_1$ is a density function over $\theta_1$ by that is (hyper-)parametrized by $\omega_1$. Note that, $f_0$ and $f_1$, and $\pi_0$ and $\pi_1$ are not necessarily probability distributions of the same type. For example $f_0$ might be a normal distribution and $f_1$ could be a completely unrelated type of probability distribution.

If we assume that $\data$ was generated either by model $\mathcal{M}_0$ or $\mathcal{M}_1$, then the probability that it was generated by $\mathcal{M}_0$ is
$$
\Prob{\mathcal{M}_0 \given \data} = \frac{\Prob{\data\given M_0}\Prob{M_0}}{\Prob{\data\given M_0}\Prob{M_0} + \Prob{\data\given M_1}\Prob{M_1}},
$$
and the relative probability that it can from $\mathcal{M}_0$ rather than $\mathcal{M}_1$ is 
$$
\underbrace{\frac{\Prob{\mathcal{M}_0 \given \data}}{\Prob{\mathcal{M}_1 \given \data}}}_{\text{Posterior odds}} 
= 
\underbrace{\frac{\Prob{\data\given M_0}}{\Prob{\data\given M_1}}}_{\text{Bayes Factor}}
\underbrace{\frac{\Prob{M_0}}{\Prob{M_1}}}_{\text{Prior odds}},
$$
with
$$
\mathrm{BF}_{\{0,1\}} \doteq \frac{\Prob{\data\given M_0}}{\Prob{\data\given M_1}} = \frac{\int \prod_{i=1}^{n}f_0(x_i\given \theta_0)\pi_0(\theta_0\given \omega_0)d\theta_0}{\int \prod_{i=1}^{n}f_1(x_i\given \theta_1)\pi_1(\theta_1\given \omega_1)d\theta_1}.
$$

We can understand the Bayes Factor in multiple ways. For example, if the prior odds is $1$, and so the two models are *a priori*, i.e., before any data is observed, identical, the the posterior odds is identical to the Bayes Factor. More generally, the Bayes Factor can be viewed as the odds ratio:
$$
\text{Bayes Factor} = \frac{\text{Posterior odds}}{\text{Prior odds}}.
$$
In other words, in general, the Bayes Factor is the factor by which the observed data changes the odds in favour of one model to another.  

Let us now consider $\data^\prime = x_{n+1}, x_{n+2} \ldots x_{n+n^\prime}$ where $x_{i^\prime} \sim f_{\textrm{True}}(x_{i^\prime} \given \phi)$ for all $i^\prime \in n+1, n+2 \ldots n+n^\prime$. In other words, $\data^\prime$ is drawn from the same, underlying generative model as $\data$.

The relative probability of $\data^\prime$ according to $\mathcal{M}_0$ versus $\mathcal{M}_1$ is[^ppf_footnote]
$$
\mathrm{PPF}_{\{0,1\}} = \frac{\Prob{\data^\prime\given \data, M_0}}{\Prob{\data^\prime\given \data, M_1}} 
=
\frac{\int \prod_{i^\prime=1}^{n^\prime}f_0(x_{i^\prime}\given \theta_0)\pi_0(\theta_0\given \data, \omega_0)d\theta_0}
     {\int \prod_{i^\prime=1}^{n^\prime}f_1(x_{i^\prime}\given \theta_1)\pi_1(\theta_1\given \data, \omega_1)d\theta_1}.
$$
Note that while $\mathrm{BF}_{\{0,1\}}$ and $\mathrm{PPF}_{\{0,1\}}$ have similar forms, the important difference is that $\mathrm{BF}_{\{0,1\}}$ is the ratio of two *prior predictive distributions* and $\mathrm{PPF}_{\{0,1\}}$ is the ratio of two two *posterior predictive distributions*.

In general,
$$
\frac{\Prob{\data\given M_0}}{\Prob{\data\given M_1}} \neq \frac{\Prob{\data^\prime\given \data, M_0}}{\Prob{\data^\prime\given \data, M_1}},
$$
and 
$$
\Prob{\data\given M_0} > \Prob{\data\given M_1} \centernot\Longleftrightarrow \Prob{\data^\prime\given \data, M_0} > \Prob{\data^\prime\given \data, M_1}.
$$

[^ppf_footnote]: Here, we use the acronym $\mathrm{PPF}$ for *posterior predictive factor*, although this term is not, to our knowledge, widely (or ever) used.

```{r, include=FALSE, cache=TRUE}
log_posterior_predictive <- function(nprime, mprime, n, m, alpha=1, beta=1){
  A <- lgamma(alpha + beta + n) - lgamma(alpha + m) - lgamma(beta + (n-m))
  B <- lgamma(alpha + mprime + m) + lgamma(beta + (nprime-mprime) + (n-m)) - lgamma(alpha + beta + nprime + n)
  A + B
}

log_posterior_predictive_null <- function(nprime) -nprime *log(2)

log.evidence <- function(m, n, alpha, beta) lbeta(m + alpha, n-m + beta) - lbeta(alpha, beta)
log.evidence.null <- function(n) -n *log(2)

log.bayes.factor <- function(m, n, alpha=1.0, beta=1.0) {
  log.evidence(m, n, alpha, beta) - log.evidence.null(n)
}

log.ppf <- function(m, n, mprime, nprime, alpha=1.0, beta=1.0){
  log_posterior_predictive(nprime, mprime, n, m, alpha, beta) - log_posterior_predictive_null(nprime)
}


n <- 250
theta <- 0.55


foo <- function(theta, n, nprime){
  m <- rbinom(1, n, prob = theta)
  mprime <- rbinom(1, nprime, prob = theta)

  c(-log.bayes.factor(m, n), 
    -log.ppf(m, n, mprime, nprime)
  )
}
set.seed(10101)

theta_range <- seq(0.4, 0.6, by=0.01)
sim_results <- Map(function(theta){replicate(1e5, foo(theta, n, n)) %>%
    t() %>% 
    apply(2, mean)}, theta_range) %>% 
  do.call(rbind,.) %>% 
  as_tibble() %>% 
  transmute(theta_true = theta_range,
            bf_01 = exp(V1), 
            ppf_01 = exp(V2)) %>% 
  mutate(bf_10 = 1/bf_01,
         ppf_10 = 1/ppf_01)
  
theta_0.55_row <- sim_results %>% filter(theta_true == 0.55)
```

# Example


Let's say that for all $i \in 1, 2 \ldots 2n$, $x_i \sim \mathrm{dbernoulli}(\theta_{\mathrm{True}})$, where $\theta_{\mathrm{True}}$ has a fixed but unknown value from the continuum $(0,1)$. We then divide these $2n$ observations into two sets $\data$ and $\data^\prime$, both of size $n$. 

We will propose two hypothetical models for both of these two data sets:
$$
M_0\colon \quad \left.
\begin{aligned}
x_i &\sim \mathrm{dbernoulli}(\theta_0),\quad\text{for $i \in 1\ldots n$},\\
\theta_0 &\sim \delta(\theta_0 - \tfrac{1}{2}),
\end{aligned}
\right.
$$
and
$$
M_1\colon \quad \left.
\begin{aligned}
x_i &\sim \mathrm{dbernoulli}(\theta_1),\quad\text{for $i \in 1\ldots n$}.\\
\theta_1 &\sim \mathrm{dbeta}(\alpha=1, \beta=1),
\end{aligned}
\right.
$$

If $\theta_{\mathrm{True}} = `r theta`$, $n = `r n`$, then, on average, $\mathrm{BF}_{\{0,1\}} = `r round(theta_0.55_row$bf_01, 3)`$, and $\mathrm{PPF}_{\{0,1\}} = `r round(theta_0.55_row$ppf_01, 3)`$ (and so $\mathrm{PPF}_{\{1,0\}} = `r round(theta_0.55_row$ppf_10, 3)`$). In the following table, we provide these results for a wider range of $\theta_{\mathrm{True}}$.
```{r,echo=F}
pander(sim_results)
```

From this table, we see that there is a range of values of $\theta_{\mathrm{True}}$, specifically 
$$
\{`r paste(sim_results %>% filter(bf_01 > 1 & ppf_10 > 1) %>% select(theta_true) %>% unlist() %>% unname(), collapse = ', ')`\},
$$ 
where the Bayes Factor favours $\mathcal{M}_0$, but $\mathcal{M}_1$ is a better predictor of the future observed data. 

# Conclusion
The Bayes Factor does not necessarily favour the model that better generalizes to future data.
